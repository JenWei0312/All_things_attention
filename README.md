Attention Mechanisms

1. Implemented: Multi-head Attention with scaled dot product from "Attention Is All You Need" paper
2. Implemented: Grouped Query Attention as in Llma models. But unlike in Llama models, my implementation still uses scaled dot product
3. In progress: Multi-Head Latent Attention as in DeepSeek models.
