{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1867d3c0-7251-4508-b99b-7893751cee77",
   "metadata": {},
   "source": [
    "# Advanced Insights: Attention Masks with KV-Caching\n",
    "\n",
    "## Key Pitfalls in Complex Attention Implementations\n",
    "\n",
    "### Dimension Evolution with Caching\n",
    "```python\n",
    "# Crucial dimension transitions in cached attention:\n",
    "[b, s, d_model] -> [b, s+cache, d_c] -> [b, s+cache, d_model] -> [b, num_h, s, d_head]\n",
    "```\n",
    "The non-obvious trap: even with growing K/V cache, attention output dimensions must match query length, not cached length.\n",
    "\n",
    "### Mask Causality with Growing Cache\n",
    "Standard causal masks break with KV-caching - they don't account for position-dependent attention patterns across cached sequences. Critical edge cases:\n",
    "- Token at position `i` must attend to `[0:start_pos+i]`\n",
    "- Naive mask extension leads to incorrect causality preservation\n",
    "- Performance impact of position-wise mask generation\n",
    "\n",
    "### Optimization Considerations\n",
    "1. Memory vs Compute tradeoff: Precomputing extended masks vs generating per position\n",
    "2. Batch dimension handling: Mask broadcasting impacts memory usage\n",
    "3. Fused attention patterns may break with custom mask handling\n",
    "\n",
    "## Debugging Strategy for Non-Obvious Cases\n",
    "Monitor these dimension transitions for subtle bugs:\n",
    "```python\n",
    "C_KV.shape      # Should grow: [b, s₁, d_c] -> [b, s₁+s₂, d_c]\n",
    "K_state.shape   # Post-projection growth affects attention patterns\n",
    "att_output.shape # Must maintain query dimensions despite K/V growth\n",
    "```\n",
    "\n",
    "## Practical Example: DeepSeek's MLA Edge Case\n",
    "In Multi-Latent Attention, the compressed KV cache introduces subtle interactions with attention masks due to:\n",
    "1. Joint compression affecting position-dependent patterns\n",
    "2. Non-standard dimension flow through compression/decompression\n",
    "3. Mask causality preservation across cached compressed states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f3f7f-6483-4adb-bf4b-7bd7259b141b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb67dc61-2bf5-457b-a231-ffacf046407e",
   "metadata": {},
   "source": [
    "### MLA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec52ea4c-02f5-4f57-a9b6-27e17bddeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "class MultiLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Multi-Head Latent Attention(MLA) Module As in DeepSeek_V2 pape\n",
    "        Key innovation from standard MHA:\n",
    "             1. Low-Rank Key-Value Joint Compression \n",
    "             2. Decoupled Rotary Position Embedding\n",
    "             \n",
    "    Args:\n",
    "        d_model:  Total dimension of the model.\n",
    "        num_head: Number of attention heads.\n",
    "        d_embed:  Embedding dimension\n",
    "        d_c:      K/V compression dimension\n",
    "        d_c1:     Q compression dimension\n",
    "        d_rotate: Dimension for Rotary Position Embedding\n",
    "        dropout:  Dropout rate for attention scores.\n",
    "        bias:     Whether to include bias in linear projections.\n",
    "\n",
    "        d_head:   Inferred from d_model//num_head\n",
    "\n",
    "    Inputs:\n",
    "        sequence: input sequence for self-attention and the query for cross-attention\n",
    "        key_value_state: input for the key, values for cross-attention\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model,             # Infer d_head from d_model\n",
    "        num_head, \n",
    "        d_embed, \n",
    "        d_c, \n",
    "        d_c1, \n",
    "        d_rotate, \n",
    "        dropout=0.1, \n",
    "        bias=True,\n",
    "        max_batch_size=32,   # For KV cache sizing\n",
    "        max_seq_len=2048     # For KV cache sizing \n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        assert d_c < d_embed, \"Compression dim should be smaller than embedding dim\"\n",
    "        assert d_c1 < d_embed, \"Query compression dim should be smaller than embedding dim\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        # Verify dimensions match up\n",
    "        assert d_model % num_head == 0, f\"d_model ({d_model}) must be divisible by num_head ({num_head})\"\n",
    "        self.d_head=d_model//num_head\n",
    "        self.d_embed = d_embed\n",
    "        self.d_c = d_c\n",
    "        self.d_c1 = d_c1\n",
    "        self.d_rotate = d_rotate\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # Linear down-projection(compression) transformations\n",
    "        self.DKV_proj = nn.Linear(d_embed, d_c, bias=bias)\n",
    "        self.DQ_proj = nn.Linear(d_embed, d_c1, bias=bias)\n",
    "        \n",
    "        # linear up-projection transformations\n",
    "        self.UQ_proj = nn.Linear(d_c1, d_model, bias=bias)\n",
    "        self.UK_proj = nn.Linear(d_c, d_model, bias=bias)\n",
    "        self.UV_proj = nn.Linear(d_c, d_model, bias=bias)\n",
    "        \n",
    "        # linear output transformations\n",
    "        self.output_proj = nn.Linear( d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head + d_rotate)) # Store as float in initialization\n",
    "\n",
    "        # Initialize C_KV cache for inference\n",
    "        self.cache_kv = torch.zeros(\n",
    "            (max_batch_size, max_seq_len, self.d_c)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        sequence, \n",
    "        key_value_states = None, \n",
    "        att_mask=None,\n",
    "        use_cache=False,\n",
    "        start_pos: int = 0\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Forward pass supporting both standard attention and cached inference\n",
    "        Input shape: [batch_size, seq_len, d_model=num_head * d_head]\n",
    "        Args:\n",
    "            sequence: Input sequence [batch_size, seq_len, d_model]\n",
    "            key_value_states: Optional states for cross-attention\n",
    "            att_mask: Optional attention mask\n",
    "            use_cache: Whether to use KV caching (for inference)\n",
    "            start_pos: Position in sequence when using KV cache\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, model_dim = sequence.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert model_dim == self.d_model, f\"Input dimension {model_dim} doesn't match model dimension {self.d_model}\"\n",
    "        if key_value_states is not None:\n",
    "            assert key_value_states.size(-1) == self.d_model, \\\n",
    "            f\"Cross attention key/value dimension {key_value_states.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "\n",
    "        # Determine kv_seq_len early\n",
    "        kv_seq_len = key_value_states.size(1) if is_cross_attention else seq_len\n",
    "        \n",
    "        # Linear projections and reshape for multi-head, in the order of Q, K/V\n",
    "        # Down and up projection for query\n",
    "        C_Q = self.DQ_proj(sequence)     #[batch_size, seq_len, d_c1]\n",
    "        Q_state = self.UQ_proj(C_Q)      #[batch_size, seq_len, d_model]\n",
    "\n",
    "    \n",
    "        if use_cache:\n",
    "            self.cache_kv = self.cache_kv.to(sequence.device)\n",
    "            \"\"\"debug\"\"\"\n",
    "            print(f\"Cache shape: {self.cache_kv.shape}\")\n",
    "            print(f\"Current token sequence length: {kv_seq_len}\")\n",
    "            print(f\"Start position: {start_pos}\")\n",
    "            # Get current compressed KV states\n",
    "            current_kv = self.DKV_proj(key_value_states if is_cross_attention else sequence) #[batch_size, kv_seq_len, d_c]\n",
    "            # Update cache using kv_seq_len instead of seq_len\n",
    "            self.cache_kv[:batch_size, start_pos:start_pos + kv_seq_len] = current_kv\n",
    "            # Use cached compressed KV up to current position\n",
    "            C_KV = self.cache_kv[:batch_size, :start_pos + kv_seq_len]\n",
    "            \n",
    "            if att_mask is not None:\n",
    "                # Get the original mask shape\n",
    "                mask_size = att_mask.size(-1)\n",
    "                cached_len = start_pos + kv_seq_len        # cached key_len, including previous key\n",
    "                assert C_KV.size(1) == cached_len, \\\n",
    "            f\"Cached key/value length {C_KV.size(1)} doesn't match theoretical length {cached_len}\"\n",
    "                \n",
    "                # Create new mask matching attention matrix shape\n",
    "                extended_mask = torch.zeros(\n",
    "                    (batch_size, 1, seq_len, cached_len),  # [batch, head, query_len, key_len]\n",
    "                    device=att_mask.device,\n",
    "                    dtype=att_mask.dtype\n",
    "                )\n",
    "                \n",
    "                # Fill in the mask appropriately - we need to be careful about the causality here\n",
    "                # For each query position, it should only attend to cached positions up to that point\n",
    "                for i in range(seq_len):\n",
    "                    extended_mask[:, :, i, :(start_pos + i + 1)] = 0  # Can attend\n",
    "                    extended_mask[:, :, i, (start_pos + i + 1):] = float('-inf')  # Cannot attend\n",
    "                    \n",
    "                att_mask = extended_mask\n",
    "        else:\n",
    "            # Compression projection for C_KV\n",
    "            C_KV = self.DKV_proj(key_value_states if is_cross_attention else sequence) #[batch_size, kv_seq_len, d_c]\n",
    "            \n",
    "\n",
    "        # Up projection for key and value\n",
    "        K_state = self.UK_proj(C_KV)               #[batch_size, kv_seq_len, d_model]\n",
    "        V_state = self.UV_proj(C_KV)               #[batch_size, kv_seq_len, d_model]\n",
    "\n",
    "        \"\"\"\n",
    "        # Debug prints\n",
    "        # After doing up-projection on cached C_KV\n",
    "        \"\"\"\n",
    "        print(f\"C_KV shape: {C_KV.shape}\")\n",
    "        print(f\"K_state shape after projection: {K_state.shape}\")\n",
    "        print(f\"Expected reshape: {[batch_size, kv_seq_len, self.num_head, self.d_head]}\")\n",
    "        \n",
    "        #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) \n",
    "\n",
    "        # After getting K_state from projection, get its actual sequence length\n",
    "        actual_kv_len = K_state.size(1)\n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        # Use actual_kv_len instead of kv_seq_len for reshaping\n",
    "        K_state = K_state.view(batch_size, actual_kv_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, actual_kv_len, self.num_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(self.att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "        assert att_output.size(0) == batch_size, \"Batch size mismatch\"\n",
    "        assert att_output.size(2) == seq_len, \"Output sequence length should match query sequence length\"\n",
    "\n",
    "\n",
    "        # Use output_seq_len instead of seq_len\n",
    "        att_output = att_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_head*self.d_head)\n",
    "            \n",
    "        # concatinate all attention heads\n",
    "        #att_output = att_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "\n",
    "        \"\"\"debug right before the output projection\"\"\"\n",
    "        print(f\"att_output shape before proj: {att_output.shape}\")\n",
    "        print(f\"output_proj weight shape: {self.output_proj.weight.shape}\\n\\n\")\n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        assert att_output.size() == (batch_size, seq_len, self.d_model), \\\n",
    "        f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03d9d-b01e-49b2-a1a0-b77328cd5aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95eebd-86b8-43b7-86da-08d409189985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d45994ab-2022-442c-9f1d-9845af7531d7",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73014927-d3d0-4552-8118-2619ac03a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_attention_mask_with_cache (__main__.TestMultiLatentAttention.test_attention_mask_with_cache)\n",
      "Test attention masking with cached KV ... ok\n",
      "test_basic_forward (__main__.TestMultiLatentAttention.test_basic_forward)\n",
      "Test basic forward pass without caching ... ok\n",
      "test_cache_initialization (__main__.TestMultiLatentAttention.test_cache_initialization)\n",
      "Test if cache is properly initialized ... ok\n",
      "test_cross_attention (__main__.TestMultiLatentAttention.test_cross_attention)\n",
      "Test cross-attention functionality ... ok\n",
      "test_sequential_caching (__main__.TestMultiLatentAttention.test_sequential_caching)\n",
      "Test sequential forward passes with caching ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.077s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache shape: torch.Size([32, 2048, 64])\n",
      "Current token sequence length: 5\n",
      "Start position: 0\n",
      "C_KV shape: torch.Size([2, 5, 64])\n",
      "K_state shape after projection: torch.Size([2, 5, 512])\n",
      "Expected reshape: [2, 5, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 5, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n",
      "Cache shape: torch.Size([32, 2048, 64])\n",
      "Current token sequence length: 1\n",
      "Start position: 5\n",
      "C_KV shape: torch.Size([2, 6, 64])\n",
      "K_state shape after projection: torch.Size([2, 6, 512])\n",
      "Expected reshape: [2, 1, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 1, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n",
      "C_KV shape: torch.Size([2, 10, 64])\n",
      "K_state shape after projection: torch.Size([2, 10, 512])\n",
      "Expected reshape: [2, 10, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 10, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n",
      "Cache shape: torch.Size([32, 2048, 64])\n",
      "Current token sequence length: 10\n",
      "Start position: 0\n",
      "C_KV shape: torch.Size([2, 10, 64])\n",
      "K_state shape after projection: torch.Size([2, 10, 512])\n",
      "Expected reshape: [2, 10, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 10, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n",
      "C_KV shape: torch.Size([2, 20, 64])\n",
      "K_state shape after projection: torch.Size([2, 20, 512])\n",
      "Expected reshape: [2, 20, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 10, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n",
      "Cache shape: torch.Size([32, 2048, 64])\n",
      "Current token sequence length: 5\n",
      "Start position: 0\n",
      "C_KV shape: torch.Size([2, 5, 64])\n",
      "K_state shape after projection: torch.Size([2, 5, 512])\n",
      "Expected reshape: [2, 5, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 5, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n",
      "Cache shape: torch.Size([32, 2048, 64])\n",
      "Current token sequence length: 1\n",
      "Start position: 5\n",
      "C_KV shape: torch.Size([2, 6, 64])\n",
      "K_state shape after projection: torch.Size([2, 6, 512])\n",
      "Expected reshape: [2, 1, 8, 64]\n",
      "att_output shape before proj: torch.Size([2, 1, 512])\n",
      "output_proj weight shape: torch.Size([512, 512])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestMultiLatentAttention(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Common dimensions for testing\n",
    "        self.d_model = 512\n",
    "        self.num_head = 8\n",
    "        self.d_embed = 512\n",
    "        self.d_c = 64  # Compression dim for K/V\n",
    "        self.d_c1 = 64  # Compression dim for Q\n",
    "        self.d_rotate = 32  # For future RoPE implementation\n",
    "        self.batch_size = 2\n",
    "        self.seq_len = 10\n",
    "        \n",
    "        # Initialize MLA\n",
    "        self.mla = MultiLatentAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_head=self.num_head,\n",
    "            d_embed=self.d_embed,\n",
    "            d_c=self.d_c,\n",
    "            d_c1=self.d_c1,\n",
    "            d_rotate=self.d_rotate\n",
    "        )\n",
    "        \n",
    "    def test_basic_forward(self):\n",
    "        \"\"\"Test basic forward pass without caching\"\"\"\n",
    "        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n",
    "        output = self.mla(x)\n",
    "        \n",
    "        # Check output shape\n",
    "        self.assertEqual(\n",
    "            output.shape, \n",
    "            (self.batch_size, self.seq_len, self.d_model),\n",
    "            \"Output shape mismatch\"\n",
    "        )\n",
    "        \n",
    "    def test_cross_attention(self):\n",
    "        \"\"\"Test cross-attention functionality\"\"\"\n",
    "        query = torch.randn(self.batch_size, self.seq_len, self.d_model)\n",
    "        kv = torch.randn(self.batch_size, self.seq_len * 2, self.d_model)  # Different seq_len\n",
    "        \n",
    "        output = self.mla(query, key_value_states=kv)\n",
    "        self.assertEqual(\n",
    "            output.shape, \n",
    "            (self.batch_size, self.seq_len, self.d_model),\n",
    "            \"Cross-attention output shape mismatch\"\n",
    "        )\n",
    "        \n",
    "    def test_cache_initialization(self):\n",
    "        \"\"\"Test if cache is properly initialized\"\"\"\n",
    "        x = torch.randn(self.batch_size, self.seq_len, self.d_model)\n",
    "        _ = self.mla(x, use_cache=True, start_pos=0)\n",
    "        \n",
    "        self.assertIsNotNone(self.mla.cache_kv)\n",
    "        self.assertEqual(\n",
    "            self.mla.cache_kv.shape[-1],\n",
    "            self.d_c,\n",
    "            \"Cache compression dimension mismatch\"\n",
    "        )\n",
    "        \n",
    "    def test_sequential_caching(self):\n",
    "        \"\"\"Test sequential forward passes with caching\"\"\"\n",
    "        # Initial sequence\n",
    "        prompt_len = 5\n",
    "        prompt = torch.randn(self.batch_size, prompt_len, self.d_model)\n",
    "        \n",
    "        # First forward pass with prompt\n",
    "        output1 = self.mla(prompt, use_cache=True, start_pos=0)\n",
    "        cached_kv_1 = self.mla.cache_kv[:, :prompt_len].clone()\n",
    "        \n",
    "        # Second forward pass with one new token\n",
    "        new_token = torch.randn(self.batch_size, 1, self.d_model)\n",
    "        output2 = self.mla(new_token, use_cache=True, start_pos=prompt_len)\n",
    "        \n",
    "        # Verify cache consistency\n",
    "        # First part of cache should remain unchanged\n",
    "        self.assertTrue(\n",
    "            torch.allclose(\n",
    "                self.mla.cache_kv[:, :prompt_len],\n",
    "                cached_kv_1,\n",
    "                rtol=1e-5\n",
    "            ),\n",
    "            \"Cache was modified for previously processed tokens\"\n",
    "        )\n",
    "        \n",
    "        # Verify new token was added to cache\n",
    "        self.assertFalse(\n",
    "            torch.allclose(\n",
    "                self.mla.cache_kv[:, prompt_len:prompt_len+1],\n",
    "                torch.zeros_like(self.mla.cache_kv[:, prompt_len:prompt_len+1]),\n",
    "                rtol=1e-5\n",
    "            ),\n",
    "            \"New token was not added to cache\"\n",
    "        )\n",
    "        \n",
    "    def test_attention_mask_with_cache(self):\n",
    "        \"\"\"Test attention masking with cached KV\"\"\"\n",
    "        seq_len = 5\n",
    "        x = torch.randn(self.batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len) * float('-inf'), \n",
    "            diagonal=1\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        # First forward pass with mask\n",
    "        output1 = self.mla(x, use_cache=True, start_pos=0, att_mask=mask)\n",
    "        \n",
    "        # Second pass with one token\n",
    "        new_token = torch.randn(self.batch_size, 1, self.d_model)\n",
    "        extended_mask = torch.triu(\n",
    "            torch.ones(seq_len + 1, seq_len + 1) * float('-inf'),\n",
    "            diagonal=1\n",
    "        ).unsqueeze(0)\n",
    "        \n",
    "        output2 = self.mla(\n",
    "            new_token,\n",
    "            use_cache=True,\n",
    "            start_pos=seq_len,\n",
    "            att_mask=extended_mask\n",
    "        )\n",
    "        \n",
    "        self.assertEqual(\n",
    "            output2.shape,\n",
    "            (self.batch_size, 1, self.d_model),\n",
    "            \"Output shape incorrect for cached attention with mask\"\n",
    "        )\n",
    "\n",
    "def run_tests():\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestMultiLatentAttention)\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(suite)\n",
    "\n",
    "# Run the tests\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bc356-f1af-4115-95cf-d7d8d4fd9211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aec7f324-fde4-4878-b2b1-6672ce8869bf",
   "metadata": {},
   "source": [
    "# Appendix --  Understanding and Debugging Attention Masks with KV-Caching\n",
    "\n",
    "This guide explores the intricate relationship between attention masks and KV-caching in transformer architectures, with a focus on debugging complex dimension mismatches. We'll use the implementation of DeepSeek's Multi-Latent Attention as a case study.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "When implementing attention mechanisms with caching, three key components interact in non-obvious ways:\n",
    "\n",
    "1. Query/Key/Value dimensions\n",
    "2. Cached states\n",
    "3. Attention masks\n",
    "\n",
    "The main challenge arises because these components have different dimensional requirements at different stages of processing.\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "### 1. Mismatched Sequence Lengths\n",
    "\n",
    "A frequent source of bugs is the mismatch between:\n",
    "- Query sequence length (new tokens only)\n",
    "- Key/Value sequence length (cached tokens + new tokens)\n",
    "- Attention mask dimensions\n",
    "\n",
    "Consider this error pattern:\n",
    "```python\n",
    "RuntimeError: The expanded size of the tensor (1) must match the existing size (6) \n",
    "at non-singleton dimension 1. Target sizes: [1, 1, 6]. Tensor sizes: [6, 6]\n",
    "```\n",
    "\n",
    "This typically indicates that your attention mask isn't accounting for the cached sequence length.\n",
    "\n",
    "### 2. Dimension Flow in Cached Attention\n",
    "\n",
    "Let's track dimensions through a typical forward pass with caching:\n",
    "\n",
    "```python\n",
    "# Initial sequence (batch=2, seq_len=5):\n",
    "C_KV shape: [2, 5, 64]          # Compressed KV\n",
    "K_state: [2, 5, 512]            # After up-projection\n",
    "att_output: [2, 5, 512]         # Final output\n",
    "\n",
    "# Cached generation (new_seq_len=1, start_pos=5):\n",
    "C_KV shape: [2, 6, 64]          # Previous 5 + new token\n",
    "K_state: [2, 6, 512]            # Full sequence up-projected\n",
    "att_output: [2, 1, 512]         # Output only for new token\n",
    "```\n",
    "\n",
    "Key insight: While K/V states grow with caching, the output dimension should match only the new query tokens.\n",
    "\n",
    "## Debugging Strategy\n",
    "\n",
    "### 1. Verify Mask Dimensions\n",
    "\n",
    "When using caching, your attention mask needs to:\n",
    "- Match the attention matrix dimensions `[batch, heads, query_len, key_len]`\n",
    "- Account for all cached tokens\n",
    "- Maintain proper causality (each position only attends to itself and previous positions)\n",
    "\n",
    "### 2. Critical Checkpoints for Debugging\n",
    "\n",
    "Add dimension checks at these key points:\n",
    "\n",
    "```python\n",
    "# 1. After retrieving cached states\n",
    "print(f\"Cache shape: {self.cache_kv.shape}\")\n",
    "print(f\"Current token sequence length: {kv_seq_len}\")\n",
    "\n",
    "# 2. After up-projection\n",
    "print(f\"K_state shape after projection: {K_state.shape}\")\n",
    "print(f\"Expected reshape: {[batch_size, kv_seq_len, num_head, d_head]}\")\n",
    "\n",
    "# 3. Before output projection\n",
    "print(f\"att_output shape before proj: {att_output.shape}\")\n",
    "```\n",
    "\n",
    "### 3. Mask Handling Solution\n",
    "\n",
    "Here's a robust approach to handle attention masks with caching:\n",
    "\n",
    "```python\n",
    "if use_cache:\n",
    "    if att_mask is not None:\n",
    "        cached_len = start_pos + kv_seq_len\n",
    "        # Create mask matching attention matrix shape\n",
    "        extended_mask = torch.zeros(\n",
    "            (batch_size, 1, seq_len, cached_len),\n",
    "            device=att_mask.device,\n",
    "            dtype=att_mask.dtype\n",
    "        )\n",
    "        \n",
    "        # Handle causality for each query position\n",
    "        for i in range(seq_len):\n",
    "            extended_mask[:, :, i, :(start_pos + i + 1)] = 0  # Can attend\n",
    "            extended_mask[:, :, i, (start_pos + i + 1):] = float('-inf')  # Cannot attend\n",
    "            \n",
    "        att_mask = extended_mask\n",
    "```\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Always Check Dimensions**: Don't assume sequence lengths. Use actual tensor dimensions when reshaping.\n",
    "   ```python\n",
    "   actual_kv_len = K_state.size(1)\n",
    "   K_state = K_state.view(batch_size, actual_kv_len, num_head, d_head)\n",
    "   ```\n",
    "\n",
    "2. **Debug with Progressive Sequence Lengths**: Test with:\n",
    "   - Single token generation\n",
    "   - Short sequences\n",
    "   - Cross-attention scenarios\n",
    "   - Full-length sequences\n",
    "\n",
    "3. **Maintain Dimensional Assertions**: Add checks to catch dimension mismatches early:\n",
    "   ```python\n",
    "   assert att_output.size(0) == batch_size, \"Batch size mismatch\"\n",
    "   assert att_output.size(1) == seq_len, \"Output sequence length mismatch\"\n",
    "   ```\n",
    "\n",
    "## Common Error Messages and Solutions\n",
    "\n",
    "1. \"Shape [...] is invalid for input of size ...\":\n",
    "   - Check if you're using assumed dimensions instead of actual tensor sizes\n",
    "   - Verify that cached sequence lengths are properly accounted for\n",
    "\n",
    "2. \"The expanded size of the tensor ... must match the existing size\":\n",
    "   - Usually indicates mask dimension mismatch\n",
    "   - Ensure mask shape accounts for both cached and new tokens\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Debugging attention masks with caching requires careful consideration of how dimensions evolve throughout the forward pass. The key is to maintain clear dimensional tracking and ensure that masks properly account for both cached and new tokens while preserving causality constraints.\n",
    "\n",
    "Remember that even experienced teams encounter these issues - the complexity of attention mechanisms means that dimension-related bugs are common and require systematic debugging approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdde76-cc5e-4d98-8632-b5a02165656d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01820b0b-3d9a-40ee-85d1-09e8c4c3b6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
