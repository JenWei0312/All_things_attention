{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac7aaa-597d-41ce-a665-477d952b5137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecb040b-5208-4321-ae7f-8763217b38df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb67dc61-2bf5-457b-a231-ffacf046407e",
   "metadata": {},
   "source": [
    "### MLA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec52ea4c-02f5-4f57-a9b6-27e17bddeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "class MultiLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Multi-Head Latent Attention Module As in DeepSeek_V2 paper\n",
    "    Args:\n",
    "        d_model:  Total dimension of the model.\n",
    "        num_head: Number of attention heads.\n",
    "        d_embed:  Embedding dimension\n",
    "        d_c:      K/V compression dimension\n",
    "        d_c1:     Q compression dimension\n",
    "        d_rotate: Dimension for Rotary Position Embedding\n",
    "        dropout:  Dropout rate for attention scores.\n",
    "        bias:     Whether to include bias in linear projections.\n",
    "\n",
    "        d_head:   Inferred from d_model//num_head\n",
    "\n",
    "    Inputs:\n",
    "        sequence: input sequence for self-attention and the query for cross-attention\n",
    "        key_value_state: input for the key, values for cross-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, d_embed, d_c, d_c1, d_rotate, dropout=0.1, bias=True): # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        assert d_c < d_embed, \"Compression dim should be smaller than embedding dim\"\n",
    "        assert d_c1 < d_embed, \"Query compression dim should be smaller than embedding dim\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        # Verify dimensions match up\n",
    "        assert d_model % num_head == 0, f\"d_model ({d_model}) must be divisible by num_head ({num_head})\"\n",
    "        self.d_head=d_model//num_head\n",
    "        self.d_embed = d_embed\n",
    "        self.d_c = d_c\n",
    "        self.d_c1 = d_c1\n",
    "        self.d_rotate = d_rotate\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # Linear down-projection(compression) transformations\n",
    "        self.DKV_proj = nn.Linear(d_embed, d_c, bias=bias)\n",
    "        self.DQ_proj = nn.Linear(d_embed, d_c1, bias=bias)\n",
    "        \n",
    "        # linear up-projection transformations\n",
    "        self.UQ_proj = nn.Linear(d_c1, d_model, bias=bias)\n",
    "        self.UK_proj = nn.Linear(d_c, d_model, bias=bias)\n",
    "        self.UV_proj = nn.Linear(d_c, d_model, bias=bias)\n",
    "        \n",
    "        # linear output transformations\n",
    "        self.output_proj = nn.Linear(num_head*self.d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        \"\"\"Needs to be updated to d_head+d_rotate after implmenting the Decoupled Rotary Position Embedding part\"\"\"\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head + d_rotate)) # Store as float in initialization\n",
    "        \n",
    "\n",
    "    def forward(self, sequence, key_value_states = None, att_mask=None):\n",
    "        \"\"\"Input shape: [batch_size, seq_len, d_model=num_head * d_head]\"\"\"\n",
    "        batch_size, seq_len, model_dim = sequence.size()\n",
    "\n",
    "        # Check only critical input dimensions\n",
    "        assert model_dim == self.d_model, f\"Input dimension {model_dim} doesn't match model dimension {self.d_model}\"\n",
    "        if key_value_states is not None:\n",
    "            assert key_value_states.size(-1) == self.d_model, \\\n",
    "            f\"Cross attention key/value dimension {key_value_states.size(-1)} doesn't match model dimension {self.d_model}\"\n",
    "\n",
    "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
    "        # for the decoder\n",
    "        is_cross_attention = key_value_states is not None\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        # Down and up projection for query\n",
    "        C_Q = self.DQ_proj(sequence)     #[batch_size, seq_len, d_c1]\n",
    "        Q_state = self.UQ_proj(C_Q)      #[batch_size, seq_len, d_model]\n",
    "    \n",
    "\n",
    "        # Down and up projection for key and value\n",
    "        if is_cross_attention:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            C_KV = self.DKV_proj(key_value_states)     #[batch_size, kv_seq_len, d_c]\n",
    "            K_state = self.UK_proj(C_KV)               #[batch_size, kv_seq_len, d_model]\n",
    "            V_state = self.UV_proj(C_KV)               #[batch_size, kv_seq_len, d_model]\n",
    "        else:\n",
    "            kv_seq_len = seq_len\n",
    "            C_KV = self.DKV_proj(sequence)             #[batch_size, kv_seq_len, d_c]\n",
    "            K_state = self.UK_proj(C_KV)               #[batch_size, seq_len, d_model]\n",
    "            V_state = self.UV_proj(C_KV)               #[batch_size, seq_len, d_model]\n",
    "\n",
    "        #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) \n",
    "            \n",
    "        # in cross-attention, key/value sequence length might be different from query sequence length\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "\n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(self.att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "    \n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        assert att_output.size() == (batch_size, seq_len, self.d_model), \\\n",
    "        f\"Final output shape {att_output.size()} incorrect\"\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce03d9d-b01e-49b2-a1a0-b77328cd5aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
